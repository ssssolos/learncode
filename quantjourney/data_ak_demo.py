""" åœ¨è·å–è´¢åŠ¡æ•°æ®æ—¶ï¼Œåœ¨tusahreæ¥å£é‡åˆ°äº†é—®é¢˜ï¼Œç§¯åˆ†ä¸å¤Ÿçš„å°´å°¬å±€é¢ï¼Œ
äºæ˜¯é‡‡ç”¨akshareçš„æœ¬åœ°è¢«è¦å‡ ä¸ªç¼“å­˜æ–¹æ¡ˆ """

import os
import akshare as ak
import baostock as bs
import pandas as pd
import numpy as np
from tqdm import tqdm
import time
import pickle
from datetime import datetime

# åˆ›å»ºè´¢åŠ¡æ•°æ®ç¼“å­˜ç›®å½•
FIN_CACHE_DIR = "data_cache/financial"
os.makedirs(FIN_CACHE_DIR, exist_ok=True)

def get_target_stocks(clean_data, all_stocks):
    """
    ä»ç°æœ‰æ•°æ®ä¸­ç²¾å‡†æå–ç›®æ ‡è‚¡ç¥¨æ± 
    :param clean_data: ä½ çš„å¤æƒæ—¥çº¿æ•°æ® (DataFrame)
    :param all_stocks: å…¨Aè‚¡åŸºç¡€æ•°æ® (DataFrame)
    :return: æœ‰æ•ˆè‚¡ç¥¨ä»£ç åˆ—è¡¨
    """
    # 1. ä»clean_dataè·å–å®é™…ä½¿ç”¨çš„è‚¡ç¥¨
    used_stocks = clean_data['ts_code'].unique().tolist()
    
    # 2. ä»all_stocksä¸­è¿‡æ»¤ST/*ST
    if 'name' in all_stocks.columns:
        all_stocks['is_st'] = all_stocks['name'].str.contains('ST|\*ST', na=False)
    else:
        # å®‰å…¨å¤„ç†ï¼šå¦‚æœall_stocksæ²¡æœ‰nameåˆ—ï¼Œä»clean_dataè·å–
        stock_names = clean_data[['ts_code', 'name']].drop_duplicates()
        all_stocks = pd.merge(all_stocks, stock_names, on='ts_code', how='left')
        all_stocks['is_st'] = all_stocks['name'].str.contains('ST|\*ST', na=False)
    
    # 3. è¿‡æ»¤é€€å¸‚è‚¡ç¥¨ (è¦æ±‚delist_dateä¸ºç©ºæˆ–æœªæ¥æ—¥æœŸ)
    if 'delist_date' in all_stocks.columns:
        all_stocks['delist_date'] = pd.to_datetime(all_stocks['delist_date'], errors='coerce')
        valid_mask = all_stocks['delist_date'].isna() | (all_stocks['delist_date'] > datetime.now())
    else:
        valid_mask = True  # æ²¡æœ‰é€€å¸‚ä¿¡æ¯æ—¶é»˜è®¤å…¨éƒ¨æœ‰æ•ˆ
    
    # 4. åˆå¹¶æ¡ä»¶
    target_stocks = all_stocks[
        (~all_stocks['is_st']) &  # éST
        (all_stocks['ts_code'].isin(used_stocks)) &  # åœ¨clean_dataä¸­å‡ºç°
        valid_mask  # æœªé€€å¸‚
    ]['ts_code'].unique().tolist()
    
    print(f"ğŸ¯ ç²¾å‡†ç›®æ ‡è‚¡ç¥¨: {len(target_stocks)}åª (ä»{len(used_stocks)}åªæ—¥çº¿è‚¡ç¥¨ä¸­ç­›é€‰)")
    return target_stocks

def get_financial_indicators_akshare(stock_code, year):
    """
    ä»AKShareè·å–å•åªè‚¡ç¥¨å¹´åº¦è´¢åŠ¡æŒ‡æ ‡
    :param stock_code: è‚¡ç¥¨ä»£ç  (å¦‚ '600000.SH')
    :param year: å¹´ä»½ (å¦‚ 2023)
    :return: è´¢åŠ¡æŒ‡æ ‡DataFrame
    """
    cache_file = os.path.join(FIN_CACHE_DIR, f"{stock_code.replace('.','_')}_{year}.pkl")
    
    # 1. æ£€æŸ¥ç¼“å­˜
    if os.path.exists(cache_file):
        with open(cache_file, 'rb') as f:
            return pickle.load(f)
    
    # 2. æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç  (AKShareä½¿ç”¨çº¯æ•°å­—+å¸‚åœºåç¼€)
    symbol = stock_code.split('.')[0]  # å»æ‰.SH/.SZ
    market = 'sh' if stock_code.endswith('SH') else 'sz'
    
    try:
        # 3. è·å–å…³é”®è´¢åŠ¡æŒ‡æ ‡ (æœé‚¦åˆ†æ+æ ¸å¿ƒæ¯”ç‡)
        df = ak.stock_financial_analysis_indicator(
            symbol=symbol,
            market=market,
            period=f"{year}"
        )
        
        if not df.empty:
            # 4. ç²¾é€‰å…³é”®å­—æ®µ (30ä¸ªæ ¸å¿ƒæŒ‡æ ‡)
            core_fields = [
                'å‡€èµ„äº§æ”¶ç›Šç‡(%)', 'æ€»èµ„äº§æŠ¥é…¬ç‡(%)', 'é”€å”®å‡€åˆ©ç‡(%)', 'é”€å”®æ¯›åˆ©ç‡(%)',
                'èµ„äº§è´Ÿå€ºç‡(%)', 'æµåŠ¨æ¯”ç‡', 'é€ŸåŠ¨æ¯”ç‡', 'å­˜è´§å‘¨è½¬ç‡(æ¬¡)', 
                'åº”æ”¶è´¦æ¬¾å‘¨è½¬ç‡(æ¬¡)', 'æ€»èµ„äº§å‘¨è½¬ç‡(æ¬¡)', 'æ¯è‚¡æ”¶ç›Š(å…ƒ)', 
                'æ¯è‚¡å‡€èµ„äº§(å…ƒ)', 'è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡(%)', 'å‡€åˆ©æ¶¦åŒæ¯”å¢é•¿ç‡(%)',
                'ç»è¥æ´»åŠ¨ç°é‡‘æµå‡€é¢åŒæ¯”å¢é•¿ç‡(%)', 'åŸºæœ¬æ¯è‚¡æ”¶ç›ŠåŒæ¯”å¢é•¿ç‡(%)',
                'å½’å±å‡€åˆ©æ¶¦åŒæ¯”å¢é•¿ç‡(%)', 'æ‰£éå‡€åˆ©æ¶¦åŒæ¯”å¢é•¿ç‡(%)',
                'æ€»èµ„äº§åŒæ¯”å¢é•¿ç‡(%)', 'å½’å±è‚¡ä¸œæƒç›ŠåŒæ¯”å¢é•¿ç‡(%)'
            ]
            
            # 5. é‡å‘½åå­—æ®µ + æ ‡å‡†åŒ–
            rename_map = {
                'å‡€èµ„äº§æ”¶ç›Šç‡(%)': 'roe',
                'æ€»èµ„äº§æŠ¥é…¬ç‡(%)': 'roa',
                'é”€å”®å‡€åˆ©ç‡(%)': 'net_profit_margin',
                'é”€å”®æ¯›åˆ©ç‡(%)': 'gross_margin',
                'èµ„äº§è´Ÿå€ºç‡(%)': 'debt_to_assets',
                'æµåŠ¨æ¯”ç‡': 'current_ratio',
                'é€ŸåŠ¨æ¯”ç‡': 'quick_ratio',
                'æ¯è‚¡æ”¶ç›Š(å…ƒ)': 'eps',
                'æ¯è‚¡å‡€èµ„äº§(å…ƒ)': 'bps',
                'å‡€åˆ©æ¶¦åŒæ¯”å¢é•¿ç‡(%)': 'netprofit_yoy',
                'è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡(%)': 'revenue_yoy'
            }
            
            # ä»…ä¿ç•™å­˜åœ¨çš„æ ¸å¿ƒå­—æ®µ
            available_fields = [f for f in core_fields if f in df.columns]
            if available_fields:
                df = df[available_fields].copy()
                df.rename(columns=rename_map, inplace=True)
                df['ts_code'] = stock_code
                df['report_year'] = year
                
                # 6. ä¿å­˜ç¼“å­˜
                with open(cache_file, 'wb') as f:
                    pickle.dump(df, f)
                
                time.sleep(1.2)  # ä¸¥æ ¼éµå®ˆAKShareé™æµ
                return df
    
    except Exception as e:
        print(f"  âš ï¸ AKShareè·å–å¤±è´¥ [{stock_code}-{year}]: {str(e)}")
    
    return pd.DataFrame()

def get_financial_backup_baostock(stock_code, year):
    """
    Baostockå¤‡ç”¨æ–¹æ¡ˆ (å½“AKShareå¤±è´¥æ—¶)
    """
    try:
        # 1. ç™»å½•Baostock
        bs.login()
        
        # 2. æ ‡å‡†åŒ–ä»£ç  (Baostockä½¿ç”¨sh.600000æ ¼å¼)
        bs_code = stock_code.replace('.SH', '.sh').replace('.SZ', '.sz')
        
        # 3. è·å–å­£åº¦è´¢åŠ¡æ•°æ® (å–Q4ä½œä¸ºå¹´æŠ¥ä»£ç†)
        rs = bs.query_performance_express_report(
            code=bs_code,
            start_date=f"{year}-01-01",
            end_date=f"{year}-12-31"
        )
        
        # 4. å¤„ç†ç»“æœ
        data = []
        while (rs.error_code == '0') & rs.next():
            data.append(rs.get_row_data())
        
        if 
            df = pd.DataFrame(data, columns=rs.fields)
            # è½¬æ¢å…³é”®å­—æ®µ
            financial_df = pd.DataFrame({
                'ts_code': [stock_code],
                'report_year': [year],
                'eps': [pd.to_numeric(df['eps'].iloc[0], errors='coerce')],
                'roe': [pd.to_numeric(df['roe'].iloc[0], errors='coerce')],
                'netprofit_yoy': [pd.to_numeric(df['netProfitYoy'].iloc[0], errors='coerce')],
                'revenue_yoy': [pd.to_numeric(df['revenueYoy'].iloc[0], errors='coerce')]
            })
            return financial_df
    
    except Exception as e:
        print(f"  âš ï¸ Baostockå¤‡ç”¨å¤±è´¥ [{stock_code}-{year}]: {str(e)}")
    
    finally:
        bs.logout()
    
    return pd.DataFrame()

def build_financial_dataset(clean_data, all_stocks, start_year=2020, end_year=2024):
    """
    æ„å»ºè´¢åŠ¡æ•°æ®é›† (æ— ç¼å¯¹æ¥ç°æœ‰clean_data)
    :param clean_ ä½ çš„å¤æƒæ—¥çº¿æ•°æ®
    :param all_stocks: å…¨Aè‚¡åŸºç¡€æ•°æ®
    :param start_year: èµ·å§‹å¹´ä»½
    :param end_year: ç»“æŸå¹´ä»½
    :return: è´¢åŠ¡æ•°æ®DataFrame
    """
    # 1. è·å–ç²¾å‡†ç›®æ ‡è‚¡ç¥¨æ± 
    target_stocks = get_target_stocks(clean_data, all_stocks)
    
    # 2. ç”Ÿæˆéœ€è¦è·å–çš„å¹´ä»½
    years = list(range(start_year, end_year + 1))
    print(f"ğŸ“… éœ€è¦è·å– {len(years)} ä¸ªå¹´ä»½: {years}")
    
    # 3. æ£€æŸ¥å·²æœ‰ç¼“å­˜
    all_financial_data = []
    failed_records = []
    
    # 4. éå†è·å–
    for stock in tqdm(target_stocks, desc="è·å–è´¢åŠ¡æ•°æ®"):
        for year in years:
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
            cache_file = os.path.join(FIN_CACHE_DIR, f"{stock.replace('.','_')}_{year}.pkl")
            if os.path.exists(cache_file):
                with open(cache_file, 'rb') as f:
                    df = pickle.load(f)
                if not df.empty:
                    all_financial_data.append(df)
                continue
            
            # å°è¯•AKShare
            df = get_financial_indicators_akshare(stock, year)
            
            # å¤‡ç”¨ï¼šå½“AKShareå¤±è´¥æ—¶å°è¯•Baostock
            if df.empty:
                print(f"  ğŸ” å°è¯•Baostockå¤‡ç”¨ [{stock}-{year}]")
                df = get_financial_backup_baostock(stock, year)
            
            # ä¿å­˜ç»“æœ
            if not df.empty:
                all_financial_data.append(df)
            else:
                failed_records.append((stock, year))
    
    # 5. åˆå¹¶æ•°æ®
    if all_financial_
        financial_df = pd.concat(all_financial_data, ignore_index=True)
        print(f"âœ… æˆåŠŸè·å–: {len(financial_df)}æ¡è´¢åŠ¡è®°å½•")
        
        # 6. è®°å½•å¤±è´¥é¡¹
        if failed_records:
            pd.DataFrame(failed_records, columns=['ts_code', 'year']).to_csv(
                os.path.join(FIN_CACHE_DIR, "failed_records.csv"), index=False
            )
            print(f"âŒ å¤±è´¥è®°å½•: {len(failed_records)}æ¡ (å·²ä¿å­˜åˆ°failed_records.csv)")
        
        return financial_df
    
    raise Exception("æœªè·å–åˆ°ä»»ä½•è´¢åŠ¡æ•°æ®ï¼è¯·æ£€æŸ¥ç½‘ç»œå’ŒAPIçŠ¶æ€")

def align_financial_with_daily(financial_df, clean_data):
    """
    å°†è´¢åŠ¡æ•°æ®ä¸æ—¥çº¿æ•°æ®å¯¹é½ (å¤„ç†å…¬å‘Šæ»åæ€§)
    :param financial_df: è´¢åŠ¡æ•°æ®
    :param clean_ æ—¥çº¿æ•°æ®
    :return: å¯¹é½åçš„DataFrame
    """
    # 1. ä¸ºè´¢åŠ¡æ•°æ®æ·»åŠ å…¬å‘Šæ—¥æœŸ (ç®€åŒ–ç‰ˆï¼šå¹´æŠ¥ç»Ÿä¸€ä¸ºæ¬¡å¹´4æœˆ30æ—¥)
    financial_df['ann_date'] = pd.to_datetime(
        financial_df['report_year'].astype(str) + '-04-30'
    )
    
    # 2. ä»…ä¿ç•™clean_dataä¸­å­˜åœ¨çš„è‚¡ç¥¨
    financial_df = financial_df[financial_df['ts_code'].isin(clean_data['ts_code'].unique())]
    
    # 3. æŒ‰è‚¡ç¥¨å’ŒæŠ¥å‘Šå¹´æ’åº
    financial_df = financial_df.sort_values(['ts_code', 'report_year'])
    
    # 4. ä¸æ—¥çº¿æ•°æ®åˆå¹¶ (å…³é”®ï¼šå·¦è¿æ¥ä¿ç•™æ‰€æœ‰æ—¥çº¿)
    merged = pd.merge_asof(
        clean_data.sort_values('trade_date'),
        financial_df.sort_values('ann_date'),
        by='ts_code',
        left_on='trade_date',
        right_on='ann_date',
        direction='backward'  # å–æœ€è¿‘çš„å·²å…¬å‘Šè´¢åŠ¡æ•°æ®
    )
    
    # 5. å‰å‘å¡«å……è´¢åŠ¡æŒ‡æ ‡ (ç›´åˆ°æ–°å…¬å‘Šå‘å¸ƒ)
    financial_cols = ['roe', 'eps', 'netprofit_yoy', 'revenue_yoy', 'debt_to_assets']
    for col in financial_cols:
        if col in merged.columns:
            merged[col] = merged.groupby('ts_code')[col].ffill()
    
    print(f"ğŸ“ˆ è´¢åŠ¡æ•°æ®å¯¹é½å®Œæˆ! è¦†ç›–ç‡: {merged[financial_cols[0]].notna().mean():.2%}")
    return merged

# ============== ä½¿ç”¨ç¤ºä¾‹ ==============
if __name__ == "__main__":
    # 1. è·å–è´¢åŠ¡æ•°æ® (åŸºäºä½ ç°æœ‰çš„clean_dataå’Œall_stocks)
    financial_data = build_financial_dataset(
        clean_data=clean_data,
        all_stocks=all_stocks,
        start_year=2020,
        end_year=2024
    )
    
    # 2. ä¿å­˜è´¢åŠ¡æ•°æ®
    financial_data.to_parquet("data_cache/financial_data.parquet")
    print("ğŸ’¾ è´¢åŠ¡æ•°æ®å·²ä¿å­˜åˆ° data_cache/financial_data.parquet")
    
    # 3. ä¸æ—¥çº¿æ•°æ®å¯¹é½
    final_dataset = align_financial_with_daily(financial_data, clean_data)
    
    # 4. ä¿å­˜æœ€ç»ˆæ•°æ®é›†
    final_dataset.to_parquet("data_cache/full_dataset_with_financial.parquet")
    print("ğŸ‰ æœ€ç»ˆæ•°æ®é›†å·²ä¿å­˜! å½¢çŠ¶:", final_dataset.shape)