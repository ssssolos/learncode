{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74815dc2",
   "metadata": {},
   "source": [
    "D1éƒ¨åˆ†å†…å®¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69536067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®è·å–æˆåŠŸ! å½¢çŠ¶: (6, 11)\n",
      "åˆ—åï¼š['ts_code', 'trade_date', 'open', 'high', 'low', 'close', 'pre_close', 'change', 'pct_chg', 'vol', 'amount']\n",
      "âœ… ä»æœ¬åœ°ç¼“å­˜åŠ è½½å…¨é‡è‚¡ç¥¨æ•°æ® (æ›´æ–°æ—¶é—´: 2025-12-23 17:05)\n",
      "å…± 947 ä¸ªäº¤æ˜“æ—¥éœ€è¦è·å–\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ä¸‹è½½æ—¥çº¿æ•°æ®: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 947/947 [09:01<00:00,  1.75it/s]\n"
     ]
    }
   ],
   "source": [
    "import tushare as ts \n",
    "ts.set_token('3d5a22139cda9c4f4e94ff39c1f26159a637b73d78ad26b1083c289c')\n",
    "\n",
    "pro = ts.pro_api()\n",
    "df = pro.query('daily', ts_code='600000.SH', start_date='20230101', end_date='20230110')\n",
    "print(f\"æ•°æ®è·å–æˆåŠŸ! å½¢çŠ¶: {df.shape}\") \n",
    "print(f\"åˆ—åï¼š{df.columns.tolist()}\")\n",
    " # è·å–æ‰€æœ‰æ›¾ä¸Šå¸‚çš„è‚¡ç¥¨ï¼ˆå«é€€å¸‚ï¼‰\n",
    "# all_stocks = pro.stock_basic(exchange='', list_status='D', fields='ts_code,symbol,name,area,industry,list_date,delist_date')\n",
    "# print(f\"å†å²è‚¡ç¥¨æ€»æ•°: {len(all_stocks)} (å«é€€å¸‚)\")\n",
    "\n",
    "# # åˆå¹¶å½“å‰ä¸Šå¸‚è‚¡ç¥¨\n",
    "# active_stocks = pro.stock_basic(exchange='', list_status='L')\n",
    "# all_stocks = pd.concat([all_stocks, active_stocks]).drop_duplicates('ts_code')\n",
    "# print(f\"âœ… å»é‡åæ€»è‚¡ç¥¨æ•°: {len(all_stocks)}\")\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "#=======================================\n",
    "#è·å–å…¨éƒ¨è‚¡ç¥¨å¹¶ç¼“å­˜ï¼Œæ¥å£é™åˆ¶\n",
    "#=======================================\n",
    "os.makedirs('data_cache', exist_ok=True)\n",
    "CACHE_FILE = 'data_cache/all_stocks_full.pkl'\n",
    "def get_all_stocks_with_cache():\n",
    "    \"\"\"æ™ºèƒ½è·å–å…¨éƒ¨è‚¡ç¥¨æ•°æ®ï¼ˆå«å·²é€€å¸‚ï¼‰ï¼Œå¸¦æœ¬åœ°ç¼“å­˜\"\"\"\n",
    "    \n",
    "    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”24å°æ—¶å†…æœ‰æ•ˆ\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        file_time = datetime.fromtimestamp(os.path.getmtime(CACHE_FILE))\n",
    "        if (datetime.now() - file_time).total_seconds() < 86400:  \n",
    "            print(f\"âœ… ä»æœ¬åœ°ç¼“å­˜åŠ è½½å…¨é‡è‚¡ç¥¨æ•°æ® (æ›´æ–°æ—¶é—´: {file_time.strftime('%Y-%m-%d %H:%M')})\")\n",
    "            return pd.read_pickle(CACHE_FILE)\n",
    "    \n",
    "    print(\"â³ æ­£åœ¨ä»Tushareè·å–å…¨é‡è‚¡ç¥¨æ•°æ®ï¼ˆå«å·²é€€å¸‚ï¼‰...\")\n",
    "    \n",
    "    try:\n",
    "        all_stocks = pro.stock_basic( exchange='', list_status='', \n",
    "            fields='ts_code,symbol,name,area,industry,list_status,list_date,delist_date'\n",
    "        )\n",
    "        \n",
    "        # ä¿å­˜åˆ°ç¼“å­˜\n",
    "        all_stocks.to_pickle(CACHE_FILE)\n",
    "        print(f\"âœ… æˆåŠŸè·å– {len(all_stocks)} åªè‚¡ç¥¨æ•°æ®ï¼ˆå«å·²é€€å¸‚ï¼‰\")\n",
    "        print(f\"ğŸ’¾ æ•°æ®å·²ç¼“å­˜è‡³: {CACHE_FILE}\")\n",
    "        return all_stocks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è·å–æ•°æ®å¤±è´¥: {str(e)}\")\n",
    "        \n",
    "        # å°è¯•åŠ è½½æ—§ç¼“å­˜ï¼ˆå³ä½¿è¿‡æœŸï¼‰\n",
    "        if os.path.exists(CACHE_FILE):\n",
    "            print(\"ğŸ”„ åŠ è½½æœ€è¿‘ä¸€æ¬¡ç¼“å­˜æ•°æ®ï¼ˆå¯èƒ½ä¸æ˜¯æœ€æ–°\\)\")\n",
    "            return pd.read_pickle(CACHE_FILE)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "all_stocks = get_all_stocks_with_cache()\n",
    "# active_stocks = all_stocks.loc(all_stocks['list_status'] == 'L')\n",
    "# delisted_stocks = all_stocks.loc(all_stocks['list_status'] == 'D')\n",
    "# pause_stocks = all_stocks.loc(all_stocks['list_status'] == 'P')\n",
    "#========================================\n",
    "#æ‰¹é‡ä¸‹è½½æ—¥çº¿æ•°æ®\n",
    "#==================================\n",
    "\n",
    "# æŒ‰ç…§æ—¥æœŸå¿«é€ŸåŠ è½½åŠåŸºç¡€ç¼“å­˜\n",
    "os.makedirs('data_cache/daily', exist_ok=True)\n",
    "\n",
    "\n",
    "def get_all_daily_data(start_date,end_date,force_refresh=False):\n",
    "\n",
    "    #ç¼“å­˜ä»¥åŠåŸºç¡€æ£€æŸ¥\n",
    "    cache_file = f\"data_cache/daily/{start_date}_{end_date}.pkl\"\n",
    "    # å¼ºåˆ¶åˆ·æ–°æˆ–æ¸…ç†æ— æ•ˆç¼“å­˜\n",
    "    if force_refresh and os.path.exists(cache_file):\n",
    "        print(f\"ğŸ§¹ æ¸…ç†æ—§ç¼“å­˜: {cache_file}\")\n",
    "        os.remove(cache_file)\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"ä»ç¼“å­˜åŠ è½½ {start_date}~{end_date} æ•°æ®\")\n",
    "        return pd.read_pickle(cache_file)\n",
    "\n",
    "    #tusahreå»ºè®®ï¼ŒæŒ‰ç…§trade_dateæ¥æå–\n",
    "    trade_cal = pro.trade_cal(exchange='', start_date=start_date, end_date=end_date)\n",
    "    trade_days = trade_cal[trade_cal.is_open == 1]['cal_date'].tolist()\n",
    "    print(f\"å…± {len(trade_days)} ä¸ªäº¤æ˜“æ—¥éœ€è¦è·å–\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for i in tqdm(trade_days, desc=\"ä¸‹è½½æ—¥çº¿æ•°æ®\"):\n",
    "           \n",
    "        try:\n",
    "            df = pro.daily(\n",
    "                trade_date=i,\n",
    "                fields=''\n",
    "            )\n",
    "            if not df.empty and 'ts_code' in df.columns:\n",
    "                all_data.append(df)\n",
    "            else:\n",
    "                print(f\"âš ï¸ è·³è¿‡æ— æ•ˆæ‰¹æ¬¡: {i} \"\n",
    "                      f\"(è¡Œæ•°: {len(df)}, åˆ—æ•°: {len(df.columns)})\")\n",
    "            time.sleep(0.3)\n",
    "        except Exception as e:\n",
    "            print(f\" æ—¥æœŸåŒºé—´ {i} è·å–å¤±è´¥: {str(e)}\")\n",
    "            time.sleep(3)\n",
    "    \n",
    "    if all_data:\n",
    "        daily_df = pd.concat(all_data, ignore_index=True)\n",
    "        if 'ts_code' not in daily_df.columns:\n",
    "            print(f\"âš ï¸ æ•°æ®ç¼ºå°‘ts_codeåˆ—! è¿”å›ç©ºDataFrame. å®é™…åˆ—: {daily_df.columns.tolist()}\")\n",
    "        # ç”¨categoryç±»å‹å‹ç¼©å†…å­˜ è¿™ä¸€æ­¥å¾ˆé‡è¦\n",
    "        daily_df['ts_code'] = daily_df['ts_code'].astype('category')\n",
    "        daily_df.to_pickle(cache_file)\n",
    "        return daily_df\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "start_date = '20220101'\n",
    "end_date = '20251201' \n",
    "daily_data = get_all_daily_data(start_date, end_date)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46487bd1",
   "metadata": {},
   "source": [
    "æ•°æ®å¤„ç†éƒ¨åˆ†ï¼Œå¤æƒå¤„ç†ï¼ŒSTè‚¡å¤„ç†ï¼Œç¼ºå¤±å€¼å¼‚å¸¸å€¼å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faaa81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è·å–4925784è¡Œæ—¥çº¿æ•°æ®ï¼Œè¦†ç›–5621åªè‚¡ç¥¨\n",
      "     ts_code trade_date   open   high    low  close  pre_close  change  \\\n",
      "0  000001.SZ   20251201  11.60  11.70  11.53  11.69      11.61    0.08   \n",
      "1  000002.SZ   20251201   5.30   5.34   5.17   5.20       5.38   -0.18   \n",
      "2  000004.SZ   20251201  11.93  12.36  11.78  12.36      11.77    0.59   \n",
      "3  000006.SZ   20251201  10.88  10.91  10.35  10.43      11.01   -0.58   \n",
      "4  000007.SZ   20251201   9.91  10.87   9.88  10.87       9.88    0.99   \n",
      "\n",
      "   pct_chg         vol       amount  \n",
      "0   0.6891  1037322.71  1205480.323  \n",
      "1  -3.3457  3212116.25  1684908.905  \n",
      "2   5.0127    92745.00   113410.422  \n",
      "3  -5.2679   421976.64   444839.931  \n",
      "4  10.0202   167432.47   175964.693  \n",
      "ğŸ§¹ æ¸…ç†æ—§ç¼“å­˜: data_cache/adj_factors/20220101_20251201.pkl\n",
      "å…± 947 ä¸ªäº¤æ˜“æ—¥éœ€è¦è·å–\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è·å–å¤æƒå› å­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 947/947 [05:16<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ts_code trade_date   open   high    low  close  pre_close  change  \\\n",
      "0  000001.SZ   20251201  11.60  11.70  11.53  11.69      11.61    0.08   \n",
      "1  000002.SZ   20251201   5.30   5.34   5.17   5.20       5.38   -0.18   \n",
      "2  000004.SZ   20251201  11.93  12.36  11.78  12.36      11.77    0.59   \n",
      "3  000006.SZ   20251201  10.88  10.91  10.35  10.43      11.01   -0.58   \n",
      "4  000007.SZ   20251201   9.91  10.87   9.88  10.87       9.88    0.99   \n",
      "\n",
      "   pct_chg         vol       amount  adj_factor    open_adj    high_adj  \\\n",
      "0   0.6891  1037322.71  1205480.323    134.5794  1561.12104  1574.57898   \n",
      "1  -3.3457  3212116.25  1684908.905    181.7040   963.03120   970.29936   \n",
      "2   5.0127    92745.00   113410.422      4.0640    48.48352    50.23104   \n",
      "3  -5.2679   421976.64   444839.931     39.7400   432.37120   433.56340   \n",
      "4  10.0202   167432.47   175964.693      8.2840    82.09444    90.04708   \n",
      "\n",
      "       low_adj    close_adj  \n",
      "0  1551.700482  1573.233186  \n",
      "1   939.409680   944.860800  \n",
      "2    47.873920    50.231040  \n",
      "3   411.309000   414.488200  \n",
      "4    81.845920    90.047080  \n"
     ]
    }
   ],
   "source": [
    "#================================================\n",
    "#å¤æƒå¤„ç†\n",
    "#================================================\n",
    "os.makedirs('data_cache/adj_factors', exist_ok=True)\n",
    "\n",
    "if 'ts_code' in daily_data.columns and not daily_data.empty:\n",
    "    print(f\"âœ… è·å–{len(daily_data)}è¡Œæ—¥çº¿æ•°æ®ï¼Œè¦†ç›–{daily_data['ts_code'].nunique()}åªè‚¡ç¥¨\")\n",
    "else:\n",
    "    print(\"âŒ æ•°æ®è·å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå“åº”\")\n",
    "print(daily_data.head())\n",
    "def get_adj_factors(daily_df,start_date,end_date,force_refresh=False):\n",
    "    \n",
    "    cache_file = f\"data_cache/adj_factors/{start_date}_{end_date}.pkl\"\n",
    "    # å¼ºåˆ¶åˆ·æ–°æˆ–æ¸…ç†æ— æ•ˆç¼“å­˜\n",
    "    if force_refresh and os.path.exists(cache_file):\n",
    "        print(f\"ğŸ§¹ æ¸…ç†æ—§ç¼“å­˜: {cache_file}\")\n",
    "        os.remove(cache_file)\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"ä»ç¼“å­˜åŠ è½½ {start_date}~{end_date} æ•°æ®\")\n",
    "        return pd.read_pickle(cache_file)\n",
    "\n",
    "    #tusahreå»ºè®®ï¼ŒæŒ‰ç…§trade_dateæ¥æå–\n",
    "    trade_cal = pro.trade_cal(exchange='', start_date=start_date, end_date=end_date)\n",
    "    trade_days = trade_cal[trade_cal.is_open == 1]['cal_date'].tolist()\n",
    "    print(f\"å…± {len(trade_days)} ä¸ªäº¤æ˜“æ—¥éœ€è¦è·å–\")\n",
    "    # è·å–å¤æƒå› å­ï¼ˆå…³é”®ï¼ï¼‰\n",
    "    adj_factors = []\n",
    "    for i in tqdm(trade_days, desc=\"è·å–å¤æƒå› å­\"):\n",
    "        try:\n",
    "            df_adj = pro.adj_factor(ts_code='',trade_date=i)\n",
    "            if not df_adj.empty and 'ts_code' in df_adj.columns:  \n",
    "                adj_factors.append(df_adj)\n",
    "            time.sleep(0.15)\n",
    "        except Exception as e:\n",
    "            print(f\"æ—¥æœŸ {i} è·å–å¤±è´¥: {str(e)}\")\n",
    "            time.sleep(3)\n",
    "\n",
    "    adj_df = pd.concat(adj_factors, ignore_index=True)\n",
    "\n",
    "    # åˆå¹¶å¤æƒå› å­\n",
    "    daily_df = pd.merge(daily_df, adj_df[['ts_code','trade_date','adj_factor']], \n",
    "                        on=['ts_code','trade_date'], how='left')\n",
    "\n",
    "    # å¤æƒè®¡ç®—\n",
    "    daily_df['open_adj'] = daily_df['open'] * daily_df['adj_factor']\n",
    "    daily_df['high_adj'] = daily_df['high'] * daily_df['adj_factor']\n",
    "    daily_df['low_adj'] = daily_df['low'] * daily_df['adj_factor']\n",
    "    daily_df['close_adj'] = daily_df['close'] * daily_df['adj_factor']\n",
    "\n",
    "    #é‡Šæ”¾å†…å­˜ä¸€ä¸‹\n",
    "    daily_df=daily_df.copy()\n",
    "    daily_df.to_pickle(cache_file)\n",
    "    return daily_df\n",
    "\n",
    "adjdaily_data = get_adj_factors(daily_data,start_date,end_date)\n",
    "print(adjdaily_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "788b27f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ts_code trade_date  open  high   low  close  pre_close  change  \\\n",
      "16297  920985.BJ   20251127  8.65  8.77  8.58   8.58       8.69   -0.11   \n",
      "\n",
      "       pct_chg       vol     amount  adj_factor  open_adj  high_adj   low_adj  \\\n",
      "16297  -1.2658  22831.79  19803.047       1.628   14.0822  14.27756  13.96824   \n",
      "\n",
      "       close_adj  \n",
      "16297   13.96824  \n"
     ]
    }
   ],
   "source": [
    "print(adjdaily_data.loc[(adjdaily_data['trade_date']=='20251127')&(adjdaily_data['ts_code']=='920985.BJ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f58110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================\n",
    "#æ•°æ®æ¸…æ´—ï¼ˆSTã€é€€å¸‚ã€ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ï¼‰\n",
    "#===========================\n",
    "\n",
    "def clean_stock_data(daily_df: pd.DataFrame, all_stocks: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ä¸“ä¸šæ•°æ®æ¸…æ´—æµæ°´çº¿\n",
    "    :param daily_df: åŸå§‹æ—¥çº¿æ•°æ®\n",
    "    :param all_stocks: åŒ…å«é€€å¸‚æ—¥æœŸçš„è‚¡ç¥¨å…ƒæ•°æ®\n",
    "    :return: æ¸…æ´—åçš„DataFrame\n",
    "    \"\"\"\n",
    "    # 1. åˆ›å»ºå·¥ä½œå‰¯æœ¬ï¼ˆä¿æŠ¤åŸå§‹æ•°æ®ï¼‰\n",
    "    df = daily_df.copy()\n",
    "    all_stocks['delist_date'] = pd.to_datetime(all_stocks['delist_date'], errors='coerce')\n",
    "    # 2. STæ ‡è®°ï¼ˆå®‰å…¨å¤„ç†*STï¼‰\n",
    "    df['is_st'] = df['name'].str.contains(r'\\*?ST', regex=True)\n",
    "    \n",
    "    # 3. æ™ºèƒ½é€€å¸‚è¿‡æ»¤\n",
    "    #   - åˆå¹¶é€€å¸‚æ—¥æœŸï¼ˆä¿ç•™æ‰€æœ‰è‚¡ç¥¨ï¼‰\n",
    "    df = pd.merge(\n",
    "        df, \n",
    "        all_stocks[['ts_code', 'delist_date']].drop_duplicates(),\n",
    "        on='ts_code', how='left'\n",
    "    )\n",
    "    \n",
    "    #   - è½¬æ¢æ—¥æœŸç±»å‹\n",
    "    df['trade_date'] = pd.to_datetime(df['trade_date'])\n",
    "    df['delist_date'] = pd.to_datetime(df['delist_date'])\n",
    "    \n",
    "    #   - åˆ›å»ºå®‰å…¨æ©ç ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼‰\n",
    "    mask = (\n",
    "        # æœªé€€å¸‚è‚¡ç¥¨ï¼šæ— é€€å¸‚æ—¥æœŸ\n",
    "        df['delist_date'].isna() |\n",
    "        # å·²é€€å¸‚è‚¡ç¥¨ï¼šåœ¨é€€å¸‚æ—¥+30å¤©å†…\n",
    "        (df['trade_date'] <= df['delist_date'] + pd.Timedelta(days=30))\n",
    "    )\n",
    "    df = df[mask]\n",
    "    \n",
    "    # 4. åˆ›å»ºæ¸…æ´—é›†ï¼ˆéST + ä¿ç•™æœ‰æ•ˆé€€å¸‚æ•°æ®ï¼‰\n",
    "    clean_df = df[~df['is_st']].copy()  # åªè¿‡æ»¤STï¼Œä¿ç•™æœ‰æ•ˆé€€å¸‚æ•°æ®\n",
    "    \n",
    "    # 5. ä¸¥æ ¼æ—¶é—´æ’åºï¼ˆæ”¶ç›Šç‡è®¡ç®—å‰æï¼‰\n",
    "    clean_df = clean_df.sort_values(['ts_code', 'trade_date']).reset_index(drop=True)\n",
    "    \n",
    "    # 6. æ”¶ç›Šç‡è®¡ç®—ï¼ˆåœ¨clean_dfä¸Šï¼ï¼‰\n",
    "    clean_df['ret'] = clean_df.groupby('ts_code')['close_adj'].pct_change()\n",
    "    \n",
    "    # 7. å®‰å…¨æç«¯å€¼è¿‡æ»¤ï¼ˆä¿ç•™é¦–æ—¥ï¼‰\n",
    "    clean_df = clean_df[\n",
    "        (clean_df['ret'].isna()) |  # ä¿ç•™é¦–æ—¥\n",
    "        (abs(clean_df['ret']) < 0.2)\n",
    "    ]\n",
    "    \n",
    "    # 8. æ™ºèƒ½ç¼ºå¤±å€¼å¤„ç†\n",
    "    #   - ä»·æ ¼ï¼šå‰å‘å¡«å……ï¼ˆåœç‰Œæ—¶ç”¨å‰å€¼ï¼‰\n",
    "    clean_df['open_adj'] = clean_df.groupby('ts_code')['open_adj'].ffill()\n",
    "    clean_df['close_adj'] = clean_df.groupby('ts_code')['close_adj'].ffill()\n",
    "    \n",
    "    #   - æˆäº¤é‡ï¼šåœç‰Œæ—¶å¡«0\n",
    "    clean_df['vol'] = clean_df.groupby('ts_code')['vol'].fillna(0)\n",
    "    \n",
    "    # 9. æ—¥æœŸæ ‡å‡†åŒ–ï¼ˆé˜²å¾¡æ€§è½¬æ¢ï¼‰\n",
    "    if not pd.api.types.is_datetime64_any_dtype(clean_df['trade_date']):\n",
    "        clean_df['trade_date'] = pd.to_datetime(clean_df['trade_date'])\n",
    "    clean_df['trade_date'] = clean_df['trade_date'].dt.strftime('%Y%m%d')\n",
    "    \n",
    "    # 10. å†…å­˜ä¼˜åŒ–\n",
    "    clean_df['ts_code'] = clean_df['ts_code'].astype('category')\n",
    "    \n",
    "    return clean_df\n",
    "\n",
    "\n",
    "clean_data = clean_stock_data(adjdaily_data, all_stocks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f948cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================\n",
    "#D2äº¤ä»˜ä¸éªŒè¯\n",
    "#==================================\n",
    "# æ£€æŸ¥åœç‰Œè‚¡ç¥¨ï¼ˆè¿ç»­3å¤©æˆäº¤é‡=0ï¼‰\n",
    "suspend = clean_data[clean_data['vol']==0].groupby('ts_code').filter(lambda x: len(x)>3)\n",
    "print(f\"åœç‰Œè¶…3å¤©è‚¡æ•°: {suspend['ts_code'].nunique()}\")\n",
    "\n",
    "clean_data.to_hdf('/content/drive/MyDrive/cleaned_daily.h5', key='daily', mode='w')\n",
    "\n",
    "# å¿…é¡»é€šè¿‡çš„æ£€æŸ¥\n",
    "assert clean_data.isnull().sum().max() == 0, \"å­˜åœ¨æœªå¤„ç†ç¼ºå¤±å€¼!\"\n",
    "assert clean_data[clean_data['close_adj'] < 0].empty, \"å‡ºç°è´Ÿä»·æ ¼!\"\n",
    "print(\"âœ… D2æ•°æ®æ¸…æ´—éªŒè¯é€šè¿‡! ä¿å­˜è‡³cleaned_daily.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6c130d",
   "metadata": {},
   "source": [
    "D3 å› å­åŸºç¡€æ•°æ®ç”Ÿæˆï¼šè·å–è´¢åŠ¡æ•°æ®ï¼Œç”ŸæˆåŸºç¡€å› å­ï¼ˆå«è¡Œä¸šå¯¹é½ï¼‰ï¼Œæ„å»ºé¢æ¿æ•°æ®é›†ï¼ˆæœ€ç»ˆäº¤ä»˜ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064eba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================\n",
    "#è·å–è´¢åŠ¡æ•°æ®ï¼ˆæˆ‘çš„ç§¯åˆ†ï¼ï¼ï¼ï¼ï¼ï¼‰\n",
    "#=================================\n",
    "def get_financial_data(df):\n",
    "    financial_cols = ['ts_code', 'ann_date', 'end_date', 'roe', 'debt_to_assets', 'total_mv']\n",
    "    financial_data = []\n",
    "\n",
    "    # æŒ‰è‚¡ç¥¨åˆ†æ‰¹ä¸‹è½½ï¼ˆé™ä½è¯·æ±‚é‡ï¼‰\n",
    "    for ts_code in tqdm(df['ts_code'].unique()[:200], desc=\"ä¸‹è½½è´¢åŠ¡æ•°æ®\"):  # å…ˆè¯•200åª\n",
    "        try:\n",
    "            df_fin = pro.fina_indicator(ts_code=ts_code, \n",
    "                                    start_date=start_date, \n",
    "                                    end_date=end_date,\n",
    "                                    fields=','.join(financial_cols))\n",
    "            if not df_fin.empty:\n",
    "                financial_data.append(df_fin)\n",
    "            time.sleep(0.3)  # è´¢åŠ¡æ•°æ®æ¥å£æ›´æ•æ„Ÿ\n",
    "        except Exception as e:\n",
    "            print(f\"è´¢åŠ¡æ•°æ®å¤±è´¥ {ts_code}: {str(e)}\")\n",
    "\n",
    "    fin_df = pd.concat(financial_data, ignore_index=True)\n",
    "    return fin_df\n",
    "\n",
    "#========================================\n",
    "#ç”ŸæˆåŸºç¡€å› å­ï¼ˆè¡Œä¸šå¯¹é½ï¼‰\n",
    "#========================================\n",
    "def get_basic_factors(clean_df,fin_df):\n",
    "    # 1. è®¡ç®—åŸºç¡€æ”¶ç›Š\n",
    "    clean_df['ret_next'] = clean_df.groupby('ts_code')['close_adj'].pct_change(-1)  # æ¬¡æ—¥æ”¶ç›Š\n",
    "\n",
    "    # 2. åˆå¹¶è´¢åŠ¡æ•°æ®ï¼ˆå…³é”®ï¼šå¯¹é½å…¬å‘Šæ—¥ï¼‰\n",
    "    fin_df['ann_date'] = pd.to_datetime(fin_df['ann_date'])\n",
    "    clean_df['trade_date'] = pd.to_datetime(clean_df['trade_date'])\n",
    "    merged_df = pd.merge_asof(clean_df.sort_values('trade_date'), \n",
    "                            fin_df.sort_values('ann_date'),\n",
    "                            by='ts_code',\n",
    "                            left_on='trade_date',\n",
    "                            right_on='ann_date',\n",
    "                            direction='backward')  # å–æœ€è¿‘å…¬å‘Š\n",
    "\n",
    "    # 3. ç”Ÿæˆç»å…¸å› å­\n",
    "    merged_df['bp'] = 1 / merged_df['pb']  # ä»·å€¼å› å­\n",
    "    merged_df['mom_20'] = merged_df.groupby('ts_code')['close_adj'].pct_change(20)  # åŠ¨é‡\n",
    "    merged_df['turnover'] = merged_df['vol'] / merged_df['total_share']  # æ¢æ‰‹ç‡\n",
    "\n",
    "    #å¯ä»¥åšä¸€äº›å› å­å¤„ç†æ¯”å¦‚å»æå€¼ï¼ˆwinsorizeï¼‰\n",
    "    \"\"\" for col in ['bp', 'mom_20']:\n",
    "            p1 = merged_df[col].quantile(0.01)\n",
    "            p99 = merged_df[col].quantile(0.99)\n",
    "            merged_df[col] = merged_df[col].clip(p1, p99) \"\"\"\n",
    "    # 4. è¡Œä¸šæ•°æ®ï¼ˆç”¨ç”³ä¸‡ä¸€çº§ï¼‰\n",
    "    industry = pro.index_member(index_code='399300.SZ')  # æ²ªæ·±300æˆåˆ†\n",
    "    merged_df = pd.merge(merged_df, industry[['ts_code','in_date','out_date','con_code']], \n",
    "                        on='ts_code', how='left')\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "#============================\n",
    "#æ„å»ºé¢æ¿æ•°æ®é›†ä¸æœ€ç»ˆéªŒè¯\n",
    "#=============================\n",
    "final_cols = ['ts_code','trade_date','open_adj','high_adj','low_adj','close_adj','vol',\n",
    "              'ret_next','bp','mom_20','turnover','industry']\n",
    "panel_df = merged_df[final_cols].copy()\n",
    "\n",
    "# é‡ç½®ç´¢å¼•ï¼ˆç¬¦åˆalphalensè¦æ±‚ï¼‰\n",
    "panel_df = panel_df.set_index(['trade_date', 'ts_code']).sort_index()\n",
    "\n",
    "# ä¿å­˜æœ€ç»ˆæ•°æ®é›†\n",
    "panel_df.to_hdf('/content/drive/MyDrive/factor_panel.h5', key='panel', mode='w')\n",
    "print(f\"âœ… æœ€ç»ˆæ•°æ®é›†: {panel_df.shape} (è¡Œ,åˆ—)\")\n",
    "\n",
    "# éªŒè¯1ï¼šæ— æœªæ¥å‡½æ•°\n",
    "sample_stock = panel_df.xs('600000.SH', level=1).head(5)\n",
    "print(\"å‰5è¡Œæ•°æ®é¢„è§ˆ:\")\n",
    "print(sample_stock[['close_adj','bp','mom_20']].head())\n",
    "\n",
    "# éªŒè¯2ï¼šå› å­åˆ†å¸ƒ\n",
    "import matplotlib.pyplot as plt\n",
    "panel_df['bp'].groupby('trade_date').mean().plot(title='å¹³å‡BPæ—¶åºå›¾')\n",
    "plt.savefig('/content/drive/MyDrive/bp_trend.png')\n",
    "\n",
    "# éªŒè¯3ï¼šç¼ºå¤±ç‡æ£€æŸ¥\n",
    "missing_rate = panel_df.isnull().mean() * 100\n",
    "print(f\"å…³é”®å­—æ®µç¼ºå¤±ç‡:\\n{missing_rate[['bp','mom_20']]}\")\n",
    "# åˆæ ¼æ ‡å‡†: <5%ï¼ˆè´¢åŠ¡æ•°æ®å¤©ç„¶ç¼ºå¤±åˆç†ï¼‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
